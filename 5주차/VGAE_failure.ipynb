{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim \n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# 데이터 불러오기 \n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Train / Validation / Test split \n",
    "transform = RandomLinkSplit(is_undirected=True, num_val = 0.05, num_test = 0.1)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "num_nodes = data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인접행렬 만들기 \n",
    "train_edge_index = train_data.edge_index\n",
    "train_adjacency_matrix = torch.zeros((num_nodes, num_nodes))\n",
    "train_adjacency_matrix[train_edge_index[0], train_edge_index[1]] = 1\n",
    "train_adjacency_matrix = (train_adjacency_matrix + np.identity(2708)).to(torch.float32)\n",
    "\n",
    "test_edge_index = test_data.edge_index\n",
    "test_adjacency_matrix = torch.zeros((num_nodes, num_nodes))\n",
    "test_adjacency_matrix[test_edge_index[0], test_edge_index[1]] = 1\n",
    "test_adjacency_matrix = (test_adjacency_matrix + np.identity(2708)).to(torch.float32)\n",
    "\n",
    "node_features = data.x\n",
    "\n",
    "\n",
    "# 그래프 데이터 만들기 \n",
    "train_edge_index = train_edge_index.tolist()\n",
    "train_a = torch.tensor(train_edge_index[0])\n",
    "train_b = torch.tensor(train_edge_index[1])\n",
    "train_graph = dgl.graph((train_a, train_b))\n",
    "train_graph = dgl.add_self_loop(train_graph)\n",
    "\n",
    "test_edge_index = test_edge_index.tolist()\n",
    "test_a = torch.tensor(test_edge_index[0])\n",
    "test_b = torch.tensor(test_edge_index[1])\n",
    "test_graph = dgl.graph((test_a, test_b))\n",
    "test_graph = dgl.add_self_loop(test_graph)\n",
    "\n",
    "train_graph = train_graph.to(device)\n",
    "test_graph = test_graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "    def __init__(self, in_dim = 1433, hidden1_dim = 32, z = 16):\n",
    "        super().__init__()\n",
    "        self.graph_conv1 = GraphConv(in_dim, hidden1_dim, activation = F.relu, allow_zero_in_degree = True)\n",
    "        self.graph_conv_average = GraphConv(hidden1_dim, z, activation = lambda x: x, allow_zero_in_degree = True)\n",
    "        self.graph_conv_log_variance = GraphConv(hidden1_dim, z, activation = lambda x: x, allow_zero_in_degree = True)\n",
    "    \n",
    "    def encoder(self, adj, features):\n",
    "        z = self.graph_conv1(adj, features)\n",
    "        average = self.graph_conv_average(adj, z)\n",
    "        log_variance = self.graph_conv_log_variance(adj, z)\n",
    "        return average, log_variance\n",
    "    \n",
    "    def reparameterization(self, average, log_variance):\n",
    "        std = torch.exp(0.5 * log_variance)\n",
    "        eps = torch.randn_like(std)\n",
    "        return average + std * eps \n",
    "    \n",
    "    def decoder(self, z):\n",
    "        new_adj = torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        return new_adj\n",
    "\n",
    "    def forward(self, adj, features):\n",
    "        average, log_variance = self.encoder(adj, features)\n",
    "        z = self.reparameterization(average, log_variance)\n",
    "        new_adj = self.decoder(z)\n",
    "        return new_adj, average, log_variance\n",
    "\n",
    "\n",
    "def loss_function(new_adj, adj, average, log_variance):\n",
    "    # Minimize 상태로 만들기 \n",
    "    binary_cross_entropy = F.binary_cross_entropy(new_adj, adj, reduction='sum')\n",
    "    KL_divergence = - 0.5 * torch.sum(1 + log_variance - average ** 2 - log_variance.exp())\n",
    "    return binary_cross_entropy + KL_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "model = VGAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.03)\n",
    "\n",
    "def train():\n",
    "    with train_graph.local_scope():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        new_adj, average, log_variance = model(train_graph, node_features)\n",
    "        loss = loss_function(new_adj, train_adjacency_matrix, average, log_variance)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "        print(\"Train Loss: {}\".format(loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with test_graph.local_scope():\n",
    "        with torch.no_grad():\n",
    "            new_adj, average, log_variance = model(test_graph, node_features)\n",
    "            loss = loss_function(new_adj, test_adjacency_matrix, average, log_variance)\n",
    "            train_loss_list.append(loss.item())\n",
    "            print(\"test Loss: {}\".format(loss.item()))\n",
    "    return new_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12965365.0\n",
      "test Loss: 13206124.0\n",
      "Train Loss: 13366417.0\n",
      "test Loss: 11380502.0\n",
      "Train Loss: 11582376.0\n",
      "test Loss: 9845955.0\n",
      "Train Loss: 9729491.0\n",
      "test Loss: 8910170.0\n",
      "Train Loss: 8776131.0\n",
      "test Loss: 8127449.0\n",
      "Train Loss: 8214009.0\n",
      "test Loss: 7274783.5\n",
      "Train Loss: 7268223.5\n",
      "test Loss: 6683510.5\n",
      "Train Loss: 6730246.5\n",
      "test Loss: 6241459.0\n",
      "Train Loss: 6236850.5\n",
      "test Loss: 5836674.0\n",
      "Train Loss: 5845098.0\n",
      "test Loss: 5700903.0\n",
      "Train Loss: 5687142.5\n",
      "test Loss: 5830991.0\n",
      "Train Loss: 5875857.5\n",
      "test Loss: 5827972.5\n",
      "Train Loss: 5850628.0\n",
      "test Loss: 5533248.0\n",
      "Train Loss: 5558041.0\n",
      "test Loss: 5390039.0\n",
      "Train Loss: 5401951.5\n",
      "test Loss: 5453655.0\n",
      "Train Loss: 5450743.5\n",
      "test Loss: 5579611.0\n",
      "Train Loss: 5550442.5\n",
      "test Loss: 5533295.0\n",
      "Train Loss: 5524754.0\n",
      "test Loss: 5389536.0\n",
      "Train Loss: 5392219.0\n",
      "test Loss: 5333457.5\n",
      "Train Loss: 5324040.0\n",
      "test Loss: 5372286.0\n",
      "Train Loss: 5370757.0\n",
      "test Loss: 5378829.0\n",
      "Train Loss: 5387594.0\n",
      "test Loss: 5335154.0\n",
      "Train Loss: 5340020.0\n",
      "test Loss: 5295309.5\n",
      "Train Loss: 5298872.0\n",
      "test Loss: 5282279.0\n",
      "Train Loss: 5282097.5\n",
      "test Loss: 5291106.5\n",
      "Train Loss: 5282699.5\n",
      "test Loss: 5284373.5\n",
      "Train Loss: 5293506.0\n",
      "test Loss: 5276439.5\n",
      "Train Loss: 5271765.0\n",
      "test Loss: 5242248.0\n",
      "Train Loss: 5242754.0\n",
      "test Loss: 5221652.5\n",
      "Train Loss: 5227334.0\n",
      "test Loss: 5232417.0\n",
      "Train Loss: 5229260.0\n",
      "test Loss: 5237822.5\n",
      "Train Loss: 5239177.5\n",
      "test Loss: 5233347.5\n",
      "Train Loss: 5229635.5\n",
      "test Loss: 5207954.0\n",
      "Train Loss: 5209836.5\n",
      "test Loss: 5186572.0\n",
      "Train Loss: 5185800.5\n",
      "test Loss: 5191164.5\n",
      "Train Loss: 5191484.0\n",
      "test Loss: 5205348.5\n",
      "Train Loss: 5212934.0\n",
      "test Loss: 5212247.0\n",
      "Train Loss: 5215897.0\n",
      "test Loss: 5192803.0\n",
      "Train Loss: 5197592.5\n",
      "test Loss: 5177513.0\n",
      "Train Loss: 5177755.5\n",
      "test Loss: 5173003.5\n",
      "Train Loss: 5171976.5\n",
      "test Loss: 5184605.0\n",
      "Train Loss: 5182218.0\n",
      "test Loss: 5191793.5\n",
      "Train Loss: 5188483.5\n",
      "test Loss: 5191513.5\n",
      "Train Loss: 5190033.0\n",
      "test Loss: 5175279.0\n",
      "Train Loss: 5178994.5\n",
      "test Loss: 5169160.0\n",
      "Train Loss: 5168717.5\n",
      "test Loss: 5164957.0\n",
      "Train Loss: 5167732.5\n",
      "test Loss: 5170797.0\n",
      "Train Loss: 5172236.5\n",
      "test Loss: 5178879.0\n",
      "Train Loss: 5181351.5\n",
      "test Loss: 5176592.5\n",
      "Train Loss: 5175557.0\n",
      "test Loss: 5176113.5\n",
      "Train Loss: 5174730.0\n",
      "test Loss: 5164860.0\n",
      "Train Loss: 5163782.5\n",
      "test Loss: 5163730.5\n",
      "Train Loss: 5163892.5\n",
      "test Loss: 5166853.5\n",
      "Train Loss: 5167436.5\n",
      "test Loss: 5169659.5\n",
      "Train Loss: 5169675.5\n",
      "test Loss: 5170936.0\n",
      "Train Loss: 5172585.5\n",
      "test Loss: 5167345.5\n",
      "Train Loss: 5166490.5\n",
      "test Loss: 5164993.5\n",
      "Train Loss: 5163691.0\n",
      "test Loss: 5165302.5\n",
      "Train Loss: 5167748.5\n",
      "test Loss: 5167684.0\n",
      "Train Loss: 5167370.0\n",
      "test Loss: 5164032.0\n",
      "Train Loss: 5165748.0\n",
      "test Loss: 5165084.0\n",
      "Train Loss: 5164959.5\n",
      "test Loss: 5164702.0\n",
      "Train Loss: 5165921.0\n",
      "test Loss: 5167456.0\n",
      "Train Loss: 5164265.5\n",
      "test Loss: 5161317.5\n",
      "Train Loss: 5160756.5\n",
      "test Loss: 5162814.5\n",
      "Train Loss: 5162645.0\n",
      "test Loss: 5163437.5\n",
      "Train Loss: 5163487.5\n",
      "test Loss: 5161287.0\n",
      "Train Loss: 5161424.0\n",
      "test Loss: 5164404.5\n",
      "Train Loss: 5160581.0\n",
      "test Loss: 5162831.0\n",
      "Train Loss: 5164282.0\n",
      "test Loss: 5160587.0\n",
      "Train Loss: 5162519.0\n",
      "test Loss: 5161461.5\n",
      "Train Loss: 5160444.0\n",
      "test Loss: 5161905.0\n",
      "Train Loss: 5161264.5\n",
      "test Loss: 5160650.5\n",
      "Train Loss: 5161621.0\n",
      "test Loss: 5162249.0\n",
      "Train Loss: 5163314.0\n",
      "test Loss: 5160507.0\n",
      "Train Loss: 5162214.5\n",
      "test Loss: 5163552.0\n",
      "Train Loss: 5162152.5\n",
      "test Loss: 5163048.5\n",
      "Train Loss: 5161573.5\n",
      "test Loss: 5162221.0\n",
      "Train Loss: 5162086.5\n",
      "test Loss: 5163681.0\n",
      "Train Loss: 5164534.5\n",
      "test Loss: 5161852.5\n",
      "Train Loss: 5164158.5\n",
      "test Loss: 5161846.5\n",
      "Train Loss: 5162462.5\n",
      "test Loss: 5161748.5\n",
      "Train Loss: 5162450.5\n",
      "test Loss: 5164633.0\n",
      "Train Loss: 5165803.0\n",
      "test Loss: 5163974.5\n",
      "Train Loss: 5162364.0\n",
      "test Loss: 5163653.5\n",
      "Train Loss: 5162561.5\n",
      "test Loss: 5160533.0\n",
      "Train Loss: 5162337.5\n",
      "test Loss: 5161520.5\n",
      "Train Loss: 5162019.0\n",
      "test Loss: 5163211.0\n",
      "Train Loss: 5164484.5\n",
      "test Loss: 5162829.5\n",
      "Train Loss: 5161800.5\n",
      "test Loss: 5163953.5\n",
      "Train Loss: 5162664.0\n",
      "test Loss: 5161466.5\n",
      "Train Loss: 5161294.0\n",
      "test Loss: 5163712.0\n",
      "Train Loss: 5164665.0\n",
      "test Loss: 5159240.5\n",
      "Train Loss: 5160962.5\n",
      "test Loss: 5162749.0\n",
      "Train Loss: 5163967.5\n",
      "test Loss: 5159712.5\n",
      "Train Loss: 5159944.5\n",
      "test Loss: 5166384.0\n",
      "Train Loss: 5164871.0\n",
      "test Loss: 5159136.0\n",
      "Train Loss: 5158824.5\n",
      "test Loss: 5160235.0\n",
      "Train Loss: 5159570.0\n",
      "test Loss: 5160007.0\n",
      "Train Loss: 5160122.0\n",
      "test Loss: 5159726.5\n",
      "Train Loss: 5161116.5\n",
      "test Loss: 5159568.5\n",
      "Train Loss: 5159981.5\n",
      "test Loss: 5160451.5\n",
      "Train Loss: 5161926.0\n",
      "test Loss: 5158166.5\n",
      "Train Loss: 5157616.0\n",
      "test Loss: 5160644.5\n",
      "Train Loss: 5162100.0\n",
      "test Loss: 5159201.0\n",
      "Train Loss: 5159913.0\n",
      "test Loss: 5159106.0\n",
      "Train Loss: 5159397.5\n",
      "test Loss: 5159685.5\n",
      "Train Loss: 5159387.5\n",
      "test Loss: 5159340.0\n",
      "Train Loss: 5159362.0\n",
      "test Loss: 5158067.5\n",
      "Train Loss: 5159142.0\n",
      "test Loss: 5160164.5\n",
      "Train Loss: 5159814.5\n",
      "test Loss: 5158587.5\n",
      "Train Loss: 5160419.0\n",
      "test Loss: 5159289.0\n",
      "Train Loss: 5159411.0\n",
      "test Loss: 5161157.0\n",
      "Train Loss: 5160169.0\n",
      "test Loss: 5161883.5\n",
      "Train Loss: 5161908.5\n",
      "test Loss: 5158398.5\n",
      "Train Loss: 5161247.5\n",
      "test Loss: 5162040.5\n",
      "Train Loss: 5161229.5\n",
      "test Loss: 5160256.0\n",
      "Train Loss: 5160215.0\n",
      "test Loss: 5159397.0\n",
      "Train Loss: 5160795.0\n",
      "test Loss: 5161086.5\n",
      "Train Loss: 5161780.0\n",
      "test Loss: 5160071.0\n",
      "Train Loss: 5161097.5\n",
      "test Loss: 5162435.0\n",
      "Train Loss: 5161272.0\n",
      "test Loss: 5159450.0\n",
      "Train Loss: 5159639.0\n",
      "test Loss: 5160550.0\n",
      "Train Loss: 5161377.5\n",
      "test Loss: 5159212.5\n",
      "Train Loss: 5159909.5\n",
      "test Loss: 5160894.0\n",
      "Train Loss: 5161408.5\n",
      "test Loss: 5160469.5\n",
      "Train Loss: 5160116.0\n",
      "test Loss: 5161732.5\n",
      "Train Loss: 5161279.5\n",
      "test Loss: 5160976.5\n",
      "Train Loss: 5159978.0\n",
      "test Loss: 5159117.0\n",
      "Train Loss: 5159957.5\n",
      "test Loss: 5158894.5\n",
      "Train Loss: 5159290.5\n",
      "test Loss: 5160417.0\n",
      "Train Loss: 5161069.0\n",
      "test Loss: 5160646.0\n",
      "Train Loss: 5159136.5\n",
      "test Loss: 5162862.5\n",
      "Train Loss: 5162168.0\n",
      "test Loss: 5160002.5\n",
      "Train Loss: 5161155.0\n",
      "test Loss: 5159727.5\n",
      "Train Loss: 5160452.0\n",
      "test Loss: 5160359.0\n",
      "Train Loss: 5160953.5\n",
      "test Loss: 5159625.5\n",
      "Train Loss: 5160397.0\n",
      "test Loss: 5158889.5\n",
      "Train Loss: 5158747.5\n",
      "test Loss: 5161104.5\n",
      "Train Loss: 5159039.0\n",
      "test Loss: 5160590.0\n",
      "Train Loss: 5161614.5\n",
      "test Loss: 5159058.5\n",
      "Train Loss: 5158581.0\n",
      "test Loss: 5160064.0\n",
      "Train Loss: 5161538.5\n",
      "test Loss: 5159804.0\n",
      "Train Loss: 5159816.5\n",
      "test Loss: 5162491.0\n",
      "Train Loss: 5163475.5\n",
      "test Loss: 5165760.5\n",
      "Train Loss: 5163358.5\n",
      "test Loss: 5161363.0\n",
      "Train Loss: 5162580.5\n",
      "test Loss: 5162606.0\n",
      "Train Loss: 5161561.5\n",
      "test Loss: 5166362.5\n",
      "Train Loss: 5167045.0\n",
      "test Loss: 5159482.0\n",
      "Train Loss: 5160073.5\n",
      "test Loss: 5163860.0\n",
      "Train Loss: 5165870.5\n",
      "test Loss: 5168030.0\n",
      "Train Loss: 5167473.5\n",
      "test Loss: 5161933.5\n",
      "Train Loss: 5162145.0\n",
      "test Loss: 5166330.0\n",
      "Train Loss: 5164756.0\n",
      "test Loss: 5166053.0\n",
      "Train Loss: 5164775.5\n",
      "test Loss: 5160806.5\n",
      "Train Loss: 5159464.0\n",
      "test Loss: 5165375.0\n",
      "Train Loss: 5164320.5\n",
      "test Loss: 5158784.5\n",
      "Train Loss: 5158976.5\n",
      "test Loss: 5162744.5\n",
      "Train Loss: 5161808.5\n",
      "test Loss: 5160359.5\n",
      "Train Loss: 5159733.0\n",
      "test Loss: 5164648.5\n",
      "Train Loss: 5163884.0\n",
      "test Loss: 5161524.5\n",
      "Train Loss: 5161360.0\n",
      "test Loss: 5161037.5\n",
      "Train Loss: 5161784.5\n",
      "test Loss: 5163355.0\n",
      "Train Loss: 5161870.5\n",
      "test Loss: 5164046.5\n",
      "Train Loss: 5164153.5\n",
      "test Loss: 5161333.5\n",
      "Train Loss: 5160597.5\n",
      "test Loss: 5160165.5\n",
      "Train Loss: 5163839.5\n",
      "test Loss: 5159787.0\n",
      "Train Loss: 5161642.0\n",
      "test Loss: 5159440.5\n",
      "Train Loss: 5162108.5\n",
      "test Loss: 5164495.5\n",
      "Train Loss: 5163065.5\n",
      "test Loss: 5158975.5\n",
      "Train Loss: 5158406.5\n",
      "test Loss: 5166807.0\n",
      "Train Loss: 5167333.0\n",
      "test Loss: 5162435.5\n",
      "Train Loss: 5161112.0\n",
      "test Loss: 5160653.0\n",
      "Train Loss: 5161565.0\n",
      "test Loss: 5160693.5\n",
      "Train Loss: 5161681.0\n",
      "test Loss: 5160370.0\n",
      "Train Loss: 5159410.5\n",
      "test Loss: 5161890.5\n",
      "Train Loss: 5162182.0\n",
      "test Loss: 5159962.5\n",
      "Train Loss: 5160197.5\n",
      "test Loss: 5162832.0\n",
      "Train Loss: 5164677.5\n",
      "test Loss: 5158551.5\n",
      "Train Loss: 5159461.5\n",
      "test Loss: 5162100.5\n",
      "Train Loss: 5163157.0\n",
      "test Loss: 5162176.0\n",
      "Train Loss: 5161417.0\n",
      "test Loss: 5160190.0\n",
      "Train Loss: 5162586.0\n",
      "test Loss: 5160053.0\n",
      "Train Loss: 5160873.5\n",
      "test Loss: 5159655.5\n",
      "Train Loss: 5160781.5\n",
      "test Loss: 5160360.0\n",
      "Train Loss: 5160796.5\n",
      "test Loss: 5161510.5\n",
      "Train Loss: 5160610.0\n",
      "test Loss: 5160397.0\n",
      "Train Loss: 5160699.5\n",
      "test Loss: 5159412.0\n",
      "Train Loss: 5160198.0\n",
      "test Loss: 5162227.5\n",
      "Train Loss: 5161470.0\n",
      "test Loss: 5159059.0\n",
      "Train Loss: 5158783.5\n",
      "test Loss: 5160691.5\n",
      "Train Loss: 5160002.5\n",
      "test Loss: 5162873.5\n",
      "Train Loss: 5162130.5\n",
      "test Loss: 5158557.0\n",
      "Train Loss: 5158679.5\n",
      "test Loss: 5161465.0\n",
      "Train Loss: 5160973.5\n",
      "test Loss: 5161108.5\n",
      "Train Loss: 5161832.0\n",
      "test Loss: 5163661.0\n",
      "Train Loss: 5163208.5\n",
      "test Loss: 5162822.0\n",
      "Train Loss: 5161038.5\n",
      "test Loss: 5160756.5\n",
      "Train Loss: 5160848.5\n",
      "test Loss: 5160324.0\n",
      "Train Loss: 5160027.5\n",
      "test Loss: 5161725.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,201):\n",
    "    train()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 5157663.5\n",
      "AUC Score: 0.5004482530894087\n",
      "Precision: 0.6376740376740376\n"
     ]
    }
   ],
   "source": [
    "new_adj = np.array(test())\n",
    "binary_adj = (new_adj >= 0.5).astype(int)\n",
    "\n",
    "test_adjacency_matrix = np.array(test_adjacency_matrix)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "auc_score = roc_auc_score(binary_adj.flatten(), test_adjacency_matrix.flatten())\n",
    "precision = precision_score(binary_adj.flatten(), test_adjacency_matrix.flatten(), average='binary')\n",
    "print(\"AUC Score:\", auc_score)\n",
    "print(\"Precision:\", precision)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64899620a4720dc65f8590bd807a147b9e0b586abbcb5f41c4838230f5e89ff5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
